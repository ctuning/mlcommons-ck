
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../img/logo_v2.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.19">
    
    
      
        <title>Sc22 scc mlperf part2 - Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="green">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#tutorial-modularizing-and-automating-mlperf-part-2" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)" class="md-header__button md-logo" aria-label="Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)" data-md-component="logo">
      
  <img src="../../img/logo_v2.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Sc22 scc mlperf part2
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/mlcommons/ck" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  HOME

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../cmx/" class="md-tabs__link">
          
  
  
    
  
  CMX/CM

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../cmx/mlperf-inference/" class="md-tabs__link">
          
  
  
    
  
  MLPerf automations

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="https://access.cKnowledge.org" class="md-tabs__link">
        
  
  
    
  
  CK Playground

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="https://github.com/mlcommons/ck/releases" class="md-tabs__link">
        
  
  
    
  
  Releases

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)" class="md-nav__button md-logo" aria-label="Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)" data-md-component="logo">
      
  <img src="../../img/logo_v2.svg" alt="logo">

    </a>
    Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mlcommons/ck" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HOME
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../cmx/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    CMX/CM
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            CMX/CM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/install/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/understanding-cmx/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Understanding CMX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/common-automation-actions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CMX commands to share and reuse artifacts with common metadata
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/specific-automation-actions.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CMX automation actions for related artifacts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/cm4mlops.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reusing CMX automations and artifacts for MLOps, DevOps and MLPerf
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/create.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Creating new artifacts and automations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/improving-cmx/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Improving CMX framework
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/motivation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Motivation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../cmx/mlperf-inference/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    MLPerf automations
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            MLPerf automations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v4.1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MLPerf inference benchmark v4.1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../cmx/mlperf-inference/v5.0/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    MLPerf inference benchmark v5.0
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            MLPerf inference benchmark v5.0
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_3_2" id="__nav_3_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Image Classification
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_2">
            <span class="md-nav__icon md-icon"></span>
            Image Classification
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ResNet50
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3_3" id="__nav_3_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Text to Image
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_3">
            <span class="md-nav__icon md-icon"></span>
            Text to Image
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_3_3_1" id="__nav_3_3_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Stable Diffusion
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_3_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_3_1">
            <span class="md-nav__icon md-icon"></span>
            Stable Diffusion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Run Commands
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_3_1_2" >
        
          
          <label class="md-nav__link" for="__nav_3_3_3_1_2" id="__nav_3_3_3_1_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Reproducibility
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="5" aria-labelledby="__nav_3_3_3_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_3_1_2">
            <span class="md-nav__icon md-icon"></span>
            Reproducibility
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SCC24
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_3_4" id="__nav_3_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    2D Object Detection
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_4">
            <span class="md-nav__icon md-icon"></span>
            2D Object Detection
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RetinaNet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_3_5" id="__nav_3_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Automotive
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_5">
            <span class="md-nav__icon md-icon"></span>
            Automotive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_5_1" >
        
          
          <label class="md-nav__link" for="__nav_3_3_5_1" id="__nav_3_3_5_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    3D Object Detection
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_3_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_5_1">
            <span class="md-nav__icon md-icon"></span>
            3D Object Detection
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PointPainting
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_6" >
        
          
          <label class="md-nav__link" for="__nav_3_3_6" id="__nav_3_3_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Medical Imaging
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_6">
            <span class="md-nav__icon md-icon"></span>
            Medical Imaging
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3d-unet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_7" >
        
          
          <label class="md-nav__link" for="__nav_3_3_7" id="__nav_3_3_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Language Processing
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_7">
            <span class="md-nav__icon md-icon"></span>
            Language Processing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_7_1" >
        
          
          <label class="md-nav__link" for="__nav_3_3_7_1" id="__nav_3_3_7_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Bert-Large
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_3_7_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_7_1">
            <span class="md-nav__icon md-icon"></span>
            Bert-Large
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/language/bert/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Run Commands
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_7_1_2" >
        
          
          <label class="md-nav__link" for="__nav_3_3_7_1_2" id="__nav_3_3_7_1_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Reproducibility
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="5" aria-labelledby="__nav_3_3_7_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_7_1_2">
            <span class="md-nav__icon md-icon"></span>
            Reproducibility
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IndySCC24
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GPT-J
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLAMA2-70B
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLAMA3-405B
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MIXTRAL-8x7B
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_8" >
        
          
          <label class="md-nav__link" for="__nav_3_3_8" id="__nav_3_3_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Recommendation
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_8">
            <span class="md-nav__icon md-icon"></span>
            Recommendation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DLRM-v2
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_9" >
        
          
          <label class="md-nav__link" for="__nav_3_3_9" id="__nav_3_3_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Graph Neural Networks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_9">
            <span class="md-nav__icon md-icon"></span>
            Graph Neural Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    R-GAT
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://access.cKnowledge.org" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CK Playground
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://github.com/mlcommons/ck/releases" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Releases
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<p>[ <a href="../README.md">Back to index</a> ]</p>
<h1 id="tutorial-modularizing-and-automating-mlperf-part-2">Tutorial: modularizing and automating MLPerf (part 2)</h1>
<details>
<summary>Click here to see the table of contents.</summary>

* [Introduction](#introduction)
* [Update CM framework and automation repository](#update-cm-framework-and-automation-repository)
* [CM automation for the MLPerf benchmark](#cm-automation-for-the-mlperf-benchmark)
  * [MLPerf inference - C++ - RetinaNet FP32 - Open Images - ONNX - CPU - Offline](#mlperf-inference---c---retinanet-fp32---open-images---onnx---cpu---offline)
    * [Summary](#summary)
  * [MLPerf inference - Python - RetinaNet FP32 - Open Images - ONNX - GPU - Offline](#mlperf-inference---python---retinanet-fp32---open-images---onnx---gpu---offline)
    * [Prepare CUDA](#prepare-cuda)
    * [Prepare Python with virtual environment](#prepare-python-with-virtual-environment)
    * [Run MLPerf inference benchmark (offline, accuracy)](#run-mlperf-inference-benchmark-offline-accuracy)
    * [Run MLPerf inference benchmark (offline, performance)](#run-mlperf-inference-benchmark-offline-performance)
    * [Summary](#summary)
  * [MLPerf inference - C++ - RetinaNet FP32 - Open Images - ONNX - GPU - Offline](#mlperf-inference---c---retinanet-fp32---open-images---onnx---gpu---offline)
    * [Summary](#summary)
  * [MLPerf inference - Python - RetinaNet FP32 - Open Images - PyTorch - CPU - Offline](#mlperf-inference---python---retinanet-fp32---open-images---pytorch---cpu---offline)
    * [Summary](#summary)
* [The next steps](#the-next-steps)
* [Authors](#authors)
* [Acknowledgments](#acknowledgments)

</details>

<h1 id="introduction">Introduction</h1>
<p>We expect that you have completed the <a href="../sc22-scc-mlperf/">1st part</a> of this tutorial 
and managed to run the MLPerf inference benchmark for object detection
with RetinaNet FP32, Open Images and ONNX runtime on a CPU target.</p>
<p>This tutorial shows you how to customize the MLPerf inference benchmark
and run it with a C++ implementation, CUDA and PyTorch.</p>
<h1 id="update-cm-framework-and-automation-repository">Update CM framework and automation repository</h1>
<p>Note that the <a href="https://github.com/mlcommons/ck">CM automation meta-framework</a> 
and the <a href="https://github.com/mlcommons/ck/tree/master/cm-mlops">repository with automation scripts </a>
are being continuously updated by the community to improve the portability and interoperability of 
all reusable components for MLOps and DevOps.</p>
<p>You can get the latest version of the CM framework and automation repository as follows
(though be careful since CM CLI and APIs may change):</p>
<div class="highlight"><pre><span></span><code>python3<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>cmind<span class="w"> </span>-U
cm<span class="w"> </span>pull<span class="w"> </span>repo<span class="w"> </span>mlcommons@ck<span class="w"> </span>--checkout<span class="o">=</span>master
</code></pre></div>
<h1 id="cm-automation-for-the-mlperf-benchmark">CM automation for the MLPerf benchmark</h1>
<h2 id="mlperf-inference-c-retinanet-fp32-open-images-onnx-cpu-offline">MLPerf inference - C++ - RetinaNet FP32 - Open Images - ONNX - CPU - Offline</h2>
<p>Let's now run a <a href="../../list_of_scripts/#app-mlperf-inference-cpp">universal and modular C++ implementation of the MLPerf inference benchmark</a> 
(developed by <a href="https://www.linkedin.com/in/hanwen-zhu-483614189">Thomas Zhu</a> during his internship at <a href="https://octoml.ai">OctoML</a>).</p>
<p>Note that CM will reuse already installed and preprocessed Open Images dataset, model and tools
from the CM cache installed during the 1st part of this tutorial while installing the ONNX runtime library with C++ bindings for your system.</p>
<p>If you want to reinstall all dependencies, you can clean the CM cache again and restart the above command:
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>rm<span class="w"> </span>cache<span class="w"> </span>-f
</code></pre></div></p>
<p>You can run C++ implementation by simply changing <code>_python</code> variation to <code>_cpp</code> variation in our high-level CM MLPerf script
that will then set up the correct dependencies and will run the C++ implementation of this script</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;app mlperf inference generic _cpp _retinanet _onnxruntime _cpu&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--adr.python.version_min<span class="o">=</span><span class="m">3</span>.8<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--adr.compiler.tags<span class="o">=</span>gcc<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--adr.openimages-preprocessed.tags<span class="o">=</span>_500<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--mode<span class="o">=</span>accuracy<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--test_query_count<span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--rerun
</code></pre></div>
<p>CM will download the ONNX binaries for your system, compile our C++ implementation with the ONNX backend
and will run the MLPerf inference benchmark. You should normally see the following output:
<div class="highlight"><pre><span></span><code>...

loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.10s).
Accumulating evaluation results...
DONE (t=0.12s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.548
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.787
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.714
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.304
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.631
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.433
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.648
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.663
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.731
mAP=54.814%

    - running time of script &quot;run,mlperf,mlcommons,accuracy,mlc,process-accuracy&quot;: 1.18 sec.
  - running time of script &quot;app,vision,language,mlcommons,mlperf,inference,reference,generic,ref&quot;: 53.81 sec.
</code></pre></div></p>
<p>You can then obtain performance using the C++ implemnentation of the MLPerf inference benchmark as follows:
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;app mlperf inference generic _cpp _retinanet _onnxruntime _cpu&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--adr.python.version_min<span class="o">=</span><span class="m">3</span>.8<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--adr.compiler.tags<span class="o">=</span>gcc<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--adr.openimages-preprocessed.tags<span class="o">=</span>_500<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--mode<span class="o">=</span>performance<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--test_query_count<span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--rerun
</code></pre></div></p>
<p>You should get the following output (QPS will depend on the speed of your machine):
<div class="highlight"><pre><span></span><code>================================================
MLPerf Results Summary
================================================
SUT name : QueueSUT
Scenario : Offline
Mode     : PerformanceOnly
Samples per second: 0.631832
Result is : VALID
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: Yes

================================================
Additional Stats
================================================
Min latency (ns)                : 14547257820
Max latency (ns)                : 15826999233
Mean latency (ns)               : 15129106642
50.00 percentile latency (ns)   : 15045448544
90.00 percentile latency (ns)   : 15826999233
95.00 percentile latency (ns)   : 15826999233
97.00 percentile latency (ns)   : 15826999233
99.00 percentile latency (ns)   : 15826999233
99.90 percentile latency (ns)   : 15826999233

================================================
Test Parameters Used
================================================
samples_per_query : 10
target_qps : 1
target_latency (ns): 0
max_async_queries : 1
min_duration (ms): 0
max_duration (ms): 0
min_query_count : 1
max_query_count : 10
qsl_rng_seed : 14284205019438841327
sample_index_rng_seed : 4163916728725999944
schedule_rng_seed : 299063814864929621
accuracy_log_rng_seed : 0
accuracy_log_probability : 0
accuracy_log_sampling_target : 0
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 64

No warnings encountered during test.

No errors encountered during test.

  - running time of script &quot;app,vision,language,mlcommons,mlperf,inference,reference,generic,ref&quot;: 50.24 sec.
rsa-key-fgg-universal@mlperf-tests-e2-x86-16-64-ubuntu
</code></pre></div></p>
<p>We plan to continue optimizing this implementation of the MLPerf inference benchmark 
together with the community across different ML engines, models, data sets and systems.</p>
<h3 id="summary">Summary</h3>
<p>You can now test the end-to-end benchmarking and submission with the C++ implementation and ONNX on CPU
using Python virtual environment as follows (just substitute "Community" with your name or organization or anything else): </p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>pull<span class="w"> </span>repo<span class="w"> </span>mlcommons@ck

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get sys-utils-cm&quot;</span><span class="w"> </span>--quiet

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;install python-venv&quot;</span><span class="w"> </span>--version<span class="o">=</span><span class="m">3</span>.10.8<span class="w"> </span>--name<span class="o">=</span>mlperf

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span>--tags<span class="o">=</span>run,mlperf,inference,generate-run-cmds,_submission,_short,_dashboard<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.python.name<span class="o">=</span>mlperf<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.python.version_min<span class="o">=</span><span class="m">3</span>.8<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.compiler.tags<span class="o">=</span>gcc<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.openimages-preprocessed.tags<span class="o">=</span>_500<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--submitter<span class="o">=</span><span class="s2">&quot;Community&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--implementation<span class="o">=</span>cpp<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--hw_name<span class="o">=</span>default<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--model<span class="o">=</span>retinanet<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--backend<span class="o">=</span>onnxruntime<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--test_query_count<span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--clean
</code></pre></div>
<p>In case of a successfull run, you should see your crowd-testing results at this <a href="https://wandb.ai/cmind/cm-mlperf-dse-testing/table?workspace=user-gfursin">live W&amp;B dashboard</a>.</p>
<h2 id="mlperf-inference-python-retinanet-fp32-open-images-onnx-gpu-offline">MLPerf inference - Python - RetinaNet FP32 - Open Images - ONNX - GPU - Offline</h2>
<h3 id="prepare-cuda">Prepare CUDA</h3>
<p>If your system has an Nvidia GPU, you can run the MLPerf inference benchmark on this GPU
using the CM automation.</p>
<p>First you need to detect CUDA and cuDNN installation using CM as follows:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get cuda&quot;</span><span class="w"> </span>--out<span class="o">=</span>json
</code></pre></div>
<p>You should see the output similar to the following one (for CUDA 11.3):
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;deps&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">  </span><span class="nt">&quot;env&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;+CPLUS_INCLUDE_PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot;/usr/local/cuda-11.3/include&quot;</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;+C_INCLUDE_PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot;/usr/local/cuda-11.3/include&quot;</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;+DYLD_FALLBACK_LIBRARY_PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">    </span><span class="nt">&quot;+LD_LIBRARY_PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">    </span><span class="nt">&quot;+PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot;/usr/local/cuda-11.3/bin&quot;</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_CACHE_TAGS&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;version-11.3&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_INSTALLED_PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/usr/local/cuda-11.3&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_PATH_BIN&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/usr/local/cuda-11.3/bin&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_PATH_INCLUDE&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/usr/local/cuda-11.3/include&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_PATH_LIB&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/usr/local/cuda-11.3/lib64&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_PATH_LIB_CUDNN&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/usr/local/cuda-11.3/lib64/libcudnn.so&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_PATH_LIB_CUDNN_EXISTS&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;yes&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_VERSION&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;11.3&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_NVCC_BIN&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nvcc&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_NVCC_BIN_WITH_PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/usr/local/cuda-11.3/bin/nvcc&quot;</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;new_env&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;+CPLUS_INCLUDE_PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot;/usr/local/cuda-11.3/include&quot;</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;+C_INCLUDE_PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot;/usr/local/cuda-11.3/include&quot;</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;+DYLD_FALLBACK_LIBRARY_PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">    </span><span class="nt">&quot;+LD_LIBRARY_PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span>
<span class="w">    </span><span class="nt">&quot;+PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot;/usr/local/cuda-11.3/bin&quot;</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_CACHE_TAGS&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;version-11.3&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_INSTALLED_PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/usr/local/cuda-11.3&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_PATH_BIN&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/usr/local/cuda-11.3/bin&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_PATH_INCLUDE&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/usr/local/cuda-11.3/include&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_PATH_LIB&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/usr/local/cuda-11.3/lib64&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_PATH_LIB_CUDNN&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/usr/local/cuda-11.3/lib64/libcudnn.so&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_PATH_LIB_CUDNN_EXISTS&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;yes&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_CUDA_VERSION&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;11.3&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_NVCC_BIN&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;nvcc&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;CM_NVCC_BIN_WITH_PATH&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/usr/local/cuda-11.3/bin/nvcc&quot;</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;new_state&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{},</span>
<span class="w">  </span><span class="nt">&quot;return&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;state&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{}</span>
<span class="p">}</span>
</code></pre></div></p>
<p>You can obtain the information about your GPU using CM as follows:
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get cuda-devices&quot;</span>
</code></pre></div></p>
<h3 id="prepare-python-with-virtual-environment">Prepare Python with virtual environment</h3>
<p>We suggest you to install Python virtual environment to avoid mixing up your local Python:
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get sys-utils-cm&quot;</span><span class="w"> </span>--quiet

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;install python-venv&quot;</span><span class="w"> </span>--version<span class="o">=</span><span class="m">3</span>.10.8<span class="w"> </span>--name<span class="o">=</span>mlperf-cuda
</code></pre></div></p>
<h3 id="run-mlperf-inference-benchmark-offline-accuracy">Run MLPerf inference benchmark (offline, accuracy)</h3>
<p>You are now ready to run the MLPerf object detection benchmark on GPU with Python virtual environment as folllows:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;app mlperf inference generic _python _retinanet _onnxruntime _cuda&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--adr.python.name<span class="o">=</span>mlperf-cuda<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--mode<span class="o">=</span>accuracy<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--test_query_count<span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--clean
</code></pre></div>
<p>This CM script will automatically find or install all dependencies
described in its <a href="https://github.com/mlcommons/ck/blob/master/cm-mlops/script/app-mlperf-inference/_cm.yaml#L61">CM meta description</a>,
aggregate all environment variables, preprocess all files and assemble the MLPerf benchmark CMD.</p>
<p>It will take a few minutes to run it and you should see the following accuracy:</p>
<div class="highlight"><pre><span></span><code>loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.02s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.09s).
Accumulating evaluation results...
DONE (t=0.11s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.548
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.787
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.714
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.304
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.631
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.433
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.648
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.663
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.731

mAP=54.814%
</code></pre></div>
<h3 id="run-mlperf-inference-benchmark-offline-performance">Run MLPerf inference benchmark (offline, performance)</h3>
<p>Let's run the MLPerf object detection on GPU while measuring performance:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;app mlperf inference generic _python _retinanet _onnxruntime _cuda&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--adr.python.name<span class="o">=</span>mlperf-cuda<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--mode<span class="o">=</span>performance<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--clean
</code></pre></div>
<p>It will run for 2-5 minutes and you should see the output similar to the following one in the end
(the QPS is the performance result of this benchmark that depends on the speed of your system):</p>
<div class="highlight"><pre><span></span><code>TestScenario.Offline qps=8.44, mean=4.7238, time=78.230, queries=660, tiles=50.0:4.8531,80.0:5.0225,90.0:5.1124,95.0:5.1658,99.0:5.2730,99.9:5.3445


================================================
MLPerf Results Summary
================================================
...

No warnings encountered during test.

No errors encountered during test.

  - running time of script &quot;app,vision,language,mlcommons,mlperf,inference,reference,generic,ref&quot;: 86.90 sec.
</code></pre></div>
<h3 id="summary_1">Summary</h3>
<p>You can now run MLPerf in the submission mode (accuracy and performance) on GPU using the following CM command with Python virtual env
(just substitute "Community" with your organization or any other identifier):</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>pull<span class="w"> </span>repo<span class="w"> </span>mlcommons@ck

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get sys-utils-cm&quot;</span><span class="w"> </span>--quiet

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;install python-venv&quot;</span><span class="w"> </span>--version<span class="o">=</span><span class="m">3</span>.10.8<span class="w"> </span>--name<span class="o">=</span>mlperf-cuda

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span>--tags<span class="o">=</span>run,mlperf,inference,generate-run-cmds,_submission,_short,_dashboard<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.python.name<span class="o">=</span>mlperf-cuda<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.python.version_min<span class="o">=</span><span class="m">3</span>.8<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.compiler.tags<span class="o">=</span>gcc<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.openimages-preprocessed.tags<span class="o">=</span>_500<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--submitter<span class="o">=</span><span class="s2">&quot;Community&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--implementation<span class="o">=</span>python<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--hw_name<span class="o">=</span>default<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--model<span class="o">=</span>retinanet<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--backend<span class="o">=</span>onnxruntime<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--device<span class="o">=</span>gpu<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--test_query_count<span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--clean
</code></pre></div>
<p>In case of a successfull run, you should see your crowd-testing results at this <a href="https://wandb.ai/cmind/cm-mlperf-dse-testing/table?workspace=user-gfursin">live W&amp;B dashboard</a>.</p>
<h2 id="mlperf-inference-c-retinanet-fp32-open-images-onnx-gpu-offline">MLPerf inference - C++ - RetinaNet FP32 - Open Images - ONNX - GPU - Offline</h2>
<h3 id="summary_2">Summary</h3>
<p>After installing and detecting CUDA using CM in the previous section, you can also 
run the C++ implementation of the MLPerf vision benchmark with CUDA as follows
(just substitute "Community" with your organization or any other identifier):</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>pull<span class="w"> </span>repo<span class="w"> </span>mlcommons@ck

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get sys-utils-cm&quot;</span><span class="w"> </span>--quiet

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;install python-venv&quot;</span><span class="w"> </span>--version<span class="o">=</span><span class="m">3</span>.10.8<span class="w"> </span>--name<span class="o">=</span>mlperf-cuda

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span>--tags<span class="o">=</span>run,mlperf,inference,generate-run-cmds,_submission,_short,_dashboard<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.python.name<span class="o">=</span>mlperf-cuda<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.python.version_min<span class="o">=</span><span class="m">3</span>.8<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.compiler.tags<span class="o">=</span>gcc<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.openimages-preprocessed.tags<span class="o">=</span>_500<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--submitter<span class="o">=</span><span class="s2">&quot;Community&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--implementation<span class="o">=</span>cpp<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--hw_name<span class="o">=</span>default<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--model<span class="o">=</span>retinanet<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--backend<span class="o">=</span>onnxruntime<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--device<span class="o">=</span>gpu<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--test_query_count<span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--clean
</code></pre></div>
<p>In case of a successfull run, you should see your crowd-testing results at this <a href="https://wandb.ai/cmind/cm-mlperf-dse-testing/table?workspace=user-gfursin">live W&amp;B dashboard</a>.</p>
<h2 id="mlperf-inference-python-retinanet-fp32-open-images-pytorch-cpu-offline">MLPerf inference - Python - RetinaNet FP32 - Open Images - PyTorch - CPU - Offline</h2>
<h3 id="summary_3">Summary</h3>
<p>You can now try to use PyTorch instead of ONNX as follows:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>pull<span class="w"> </span>repo<span class="w"> </span>mlcommons@ck

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get sys-utils-cm&quot;</span><span class="w"> </span>--quiet

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;install python-venv&quot;</span><span class="w"> </span>--version<span class="o">=</span><span class="m">3</span>.10.8<span class="w"> </span>--name<span class="o">=</span>mlperf

cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span>--tags<span class="o">=</span>run,mlperf,inference,generate-run-cmds,_submission,_short,_dashboard<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.python.name<span class="o">=</span>mlperf<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.python.version_min<span class="o">=</span><span class="m">3</span>.8<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.compiler.tags<span class="o">=</span>gcc<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.ml-engine-torchvision.version_max<span class="o">=</span><span class="m">0</span>.12.1<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--adr.openimages-preprocessed.tags<span class="o">=</span>_500<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--submitter<span class="o">=</span><span class="s2">&quot;Community&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--implementation<span class="o">=</span>python<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--hw_name<span class="o">=</span>default<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--model<span class="o">=</span>retinanet<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--backend<span class="o">=</span>onnxruntime<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--test_query_count<span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--num_threads<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--clean
</code></pre></div>
<p>CM will install PyTorch and PyTorch Vision &lt;= 0.12.1 (we need that because current MLPerf inference implementation 
fails with other PyTorch Vision version - this will be fixed by the MLCommons inference WG)
and will run this benchmark with 1 thread (this is needed because the current PyTorch implementation 
sometimes fail with a high number of threads - this will be fixed by the MLCommons inference WG)</p>
<p>In case of a successfull run, you should see your crowd-testing results at this <a href="https://wandb.ai/cmind/cm-mlperf-dse-testing/table?workspace=user-gfursin">live W&amp;B dashboard</a>.</p>
<h1 id="the-next-steps">The next steps</h1>
<p>Please check other parts of this tutorial to learn how to 
customize and optimize MLPerf inference benchmark using MLCommons CM
(under preparation):</p>
<ul>
<li><a href="../sc22-scc-mlperf/">1st part</a>: customize MLPerf inference (Python ref implementation, Open images, ONNX, CPU)</li>
<li><a href="../sc22-scc-mlperf-part3/">3rd part</a>: customize MLPerf inference (ResNet50 Int8, ImageNet, TVM)</li>
<li><em>To be continued</em></li>
</ul>
<p>You are welcome to join the <a href="../taksforce.md">open MLCommons taskforce on automation and reproducibility</a>
to contribute to this project and continue optimizing this benchmark and prepare an official submission 
for MLPerf inference v3.0 (March 2023) with the help of the community.</p>
<p>See the development roadmap <a href="https://github.com/mlcommons/ck/issues/536">here</a>.</p>
<h1 id="authors">Authors</h1>
<ul>
<li><a href="https://cKnowledge.org/gfursin">Grigori Fursin</a> (cTuning foundation and cKnowledge.org)</li>
<li><a href="https://www.linkedin.com/in/arjunsuresh">Arjun Suresh</a> (cTuning foundation and cKnowledge.org)</li>
</ul>
<h1 id="acknowledgments">Acknowledgments</h1>
<p>We thank 
<a href="https://www.nersc.gov/about/nersc-staff/advanced-technologies-group/hai-ah-nam">Hai Ah Nam</a>,
<a href="https://www.linkedin.com/in/steve-leak">Steve Leak</a>,
<a href="https://scholar.harvard.edu/vijay-janapa-reddi/home">Vijay Janappa Reddi</a>,
<a href="https://scholar.google.com/citations?user=L_1FmIMAAAAJ&amp;hl=en">Tom Jablin</a>,
<a href="https://www.linkedin.com/in/ramesh-chukka-74b5b21">Ramesh N Chukka</a>,
<a href="https://www.linkedin.com/in/peter-mattson-33b8863/">Peter Mattson</a>,
<a href="https://www.linkedin.com/in/kanterd">David Kanter</a>,
<a href="https://www.linkedin.com/in/pablo-gonzalez-mesa-952ab2207">Pablo Gonzalez Mesa</a>,
<a href="https://www.linkedin.com/in/hanwen-zhu-483614189">Thomas Zhu</a>,
<a href="https://www.linkedin.com/in/tschmid">Thomas Schmid</a>
and <a href="https://www.linkedin.com/in/grverma">Gaurav Verma</a>
for their suggestions and contributions.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["content.tabs.link", "content.code.copy", "navigation.expand", "navigation.sections", "navigation.indexes", "navigation.instant", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "toc.follow"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
    
  </body>
</html>