
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../img/logo_v2.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.19">
    
    
      
        <title>Tutorial - Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="green">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#tutorial-modularizing-and-automating-mlperf-inference-language-processing-model-bert" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)" class="md-header__button md-logo" aria-label="Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)" data-md-component="logo">
      
  <img src="../../../../img/logo_v2.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Tutorial
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/mlcommons/ck" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
  
    
  
  HOME

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../cmx/" class="md-tabs__link">
          
  
  
    
  
  CMX/CM

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../cmx/mlperf-inference/" class="md-tabs__link">
          
  
  
    
  
  MLPerf automations

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="https://access.cKnowledge.org" class="md-tabs__link">
        
  
  
    
  
  CK Playground

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="https://github.com/mlcommons/ck/releases" class="md-tabs__link">
        
  
  
    
  
  Releases

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)" class="md-nav__button md-logo" aria-label="Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)" data-md-component="logo">
      
  <img src="../../../../img/logo_v2.svg" alt="logo">

    </a>
    Collective Knowledge (CK) and Collective Mind (CM/MLCFlow/CMX automations)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mlcommons/ck" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    HOME
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../cmx/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    CMX/CM
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            CMX/CM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/install/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/understanding-cmx/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Understanding CMX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/common-automation-actions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CMX commands to share and reuse artifacts with common metadata
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/specific-automation-actions.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CMX automation actions for related artifacts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/cm4mlops.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reusing CMX automations and artifacts for MLOps, DevOps and MLPerf
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/create.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Creating new artifacts and automations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/improving-cmx/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Improving CMX framework
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/motivation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Motivation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../cmx/mlperf-inference/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    MLPerf automations
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            MLPerf automations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v4.1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MLPerf inference benchmark v4.1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../cmx/mlperf-inference/v5.0/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    MLPerf inference benchmark v5.0
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            MLPerf inference benchmark v5.0
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_3_2" id="__nav_3_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Image Classification
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_2">
            <span class="md-nav__icon md-icon"></span>
            Image Classification
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/image_classification/resnet50/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ResNet50
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3_3" id="__nav_3_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Text to Image
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_3">
            <span class="md-nav__icon md-icon"></span>
            Text to Image
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_3_3_1" id="__nav_3_3_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Stable Diffusion
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_3_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_3_1">
            <span class="md-nav__icon md-icon"></span>
            Stable Diffusion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/text_to_image/sdxl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Run Commands
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_3_1_2" >
        
          
          <label class="md-nav__link" for="__nav_3_3_3_1_2" id="__nav_3_3_3_1_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Reproducibility
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="5" aria-labelledby="__nav_3_3_3_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_3_1_2">
            <span class="md-nav__icon md-icon"></span>
            Reproducibility
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/text_to_image/reproducibility/scc24/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    SCC24
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_3_4" id="__nav_3_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    2D Object Detection
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_4">
            <span class="md-nav__icon md-icon"></span>
            2D Object Detection
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/object_detection/retinanet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RetinaNet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_3_5" id="__nav_3_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Automotive
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_5">
            <span class="md-nav__icon md-icon"></span>
            Automotive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_5_1" >
        
          
          <label class="md-nav__link" for="__nav_3_3_5_1" id="__nav_3_3_5_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    3D Object Detection
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_3_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_5_1">
            <span class="md-nav__icon md-icon"></span>
            3D Object Detection
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/automotive/3d_object_detection/pointpainting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PointPainting
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_6" >
        
          
          <label class="md-nav__link" for="__nav_3_3_6" id="__nav_3_3_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Medical Imaging
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_6">
            <span class="md-nav__icon md-icon"></span>
            Medical Imaging
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/medical_imaging/3d-unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3d-unet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_7" >
        
          
          <label class="md-nav__link" for="__nav_3_3_7" id="__nav_3_3_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Language Processing
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_7">
            <span class="md-nav__icon md-icon"></span>
            Language Processing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_7_1" >
        
          
          <label class="md-nav__link" for="__nav_3_3_7_1" id="__nav_3_3_7_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Bert-Large
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_3_7_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_7_1">
            <span class="md-nav__icon md-icon"></span>
            Bert-Large
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/language/bert/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Run Commands
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_7_1_2" >
        
          
          <label class="md-nav__link" for="__nav_3_3_7_1_2" id="__nav_3_3_7_1_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Reproducibility
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="5" aria-labelledby="__nav_3_3_7_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_7_1_2">
            <span class="md-nav__icon md-icon"></span>
            Reproducibility
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/language/reproducibility/indyscc24-bert/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    IndySCC24
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/language/gpt-j/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GPT-J
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/language/llama2-70b/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLAMA2-70B
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/language/llama3_1-405b/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLAMA3-405B
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/language/mixtral-8x7b/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MIXTRAL-8x7B
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_8" >
        
          
          <label class="md-nav__link" for="__nav_3_3_8" id="__nav_3_3_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Recommendation
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_8">
            <span class="md-nav__icon md-icon"></span>
            Recommendation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/recommendation/dlrm-v2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DLRM-v2
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3_9" >
        
          
          <label class="md-nav__link" for="__nav_3_3_9" id="__nav_3_3_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Graph Neural Networks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_9">
            <span class="md-nav__icon md-icon"></span>
            Graph Neural Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cmx/mlperf-inference/v5.0/benchmarks/graph/rgat/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    R-GAT
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://access.cKnowledge.org" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CK Playground
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://github.com/mlcommons/ck/releases" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Releases
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<p>[ <a href="../">Back to index</a> ]</p>
<p><em>Note: from Feb 2024, we suggest you to use <a href="https://access.cknowledge.org/playground/?action=howtorun&amp;bench_uid=39877bb63fb54725">this GUI</a>
 to configure MLPerf inference benchmark, generate CM commands to run it across different implementations, models, data sets, software
 and hardware, and prepare your submissions.</em></p>
<h1 id="tutorial-modularizing-and-automating-mlperf-inference-language-processing-model-bert">Tutorial: modularizing and automating MLPerf Inference Language Processing model bert</h1>
<h1 id="introduction">Introduction</h1>
<p>It should take less than an hour to complete this tutorial. In the end, you should obtain a tarball (<code>mlperf_submission.tar.gz</code>) with the MLPerf-compatible results.</p>
<p><em>Note that both MLPerf and CM automation are evolving projects.
 If you encounter issues or have questions, please submit them <a href="https://github.com/mlcommons/ck/issues">here</a>
 and feel free to join our <a href="https://discord.gg/8jbEM4J6Ff">public discord channel</a>.</em></p>
<h1 id="system-preparation">System preparation</h1>
<h2 id="minimal-system-requirements">Minimal system requirements</h2>
<ul>
<li>CPU: 1 node (x86-64 or Arm64)</li>
<li>OS: we have tested this automation on Ubuntu 20.04, Ubuntu 22.04, Debian 10, Red Hat 9 and MacOS 13</li>
<li>Disk space: ~ 10GB</li>
<li>Python: 3.8+</li>
<li>All other dependencies (artifacts and tools) will be installed by the CM meta-framework</li>
</ul>
<h2 id="system-requirements-to-run-mlperf-on-nvidia-gpu">System requirements to run MLPerf on Nvidia GPU</h2>
<ul>
<li>GPU: any Nvidia GPU with 8GB+ or memory</li>
<li>Disk space: ~ 30GB</li>
</ul>
<h1 id="mlcommons-cm-automation-meta-framework">MLCommons CM automation meta-framework</h1>
<p>The MLCommons is developing an open-source and technology-neutral 
<a href="https://github.com/mlcommons/ck">Collective Mind meta-framework (CM)</a>
to modularize ML Systems and automate their benchmarking, optimization 
and design space exploration across continuously changing software, hardware and data.</p>
<p>CM is the second generation of the <a href="https://doi.org/10.1098/rsta.2020.0211">MLCommons CK workflow automation framework</a> 
that was originally developed to make it easier to <a href="https://learning.acm.org/techtalks/reproducibility">reproduce research papers at ML and Systems conferences</a>.
The goal is to help researchers unify and automate all the steps to prepare and run MLPerf and other benchmarks
across diverse ML models, datasets, frameworks, compilers and hardware (see <a href="https://doi.org/10.5281/zenodo.6475385">HPCA'22 presentation</a> about our motivation).</p>
<h2 id="cm-installation">CM installation</h2>
<p>Follow <a href="https://github.com/mlcommons/ck/blob/master/docs/installation.md">this guide</a> to install the MLCommons CM automation language on your system.</p>
<p>After the installation, you should be able to access the CM command line as follows:</p>
<div class="highlight"><pre><span></span><code>cm
</code></pre></div>
<div class="highlight"><pre><span></span><code>cm {action} {automation} {artifact(s)} {--flags} @input.yaml @input.json
</code></pre></div>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>--version
</code></pre></div>
<div class="highlight"><pre><span></span><code>1.5.3
</code></pre></div>
<p><em>Our goal is to keep CM language and scripts backward compatible.</em></p>
<h2 id="pull-cm-repository-with-cross-platform-mlops-and-devops-scripts">Pull CM repository with cross-platform MLOps and DevOps scripts</h2>
<p>Pull stable MLCommons CM repository with <a href="../list_of_scripts.md">cross-platform CM scripts for modular ML Systems</a>:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>pull<span class="w"> </span>repo<span class="w"> </span>mlcommons@ck
</code></pre></div>
<p>CM pulls all such repositories into the <code>$HOME/CM</code> directory to search for CM automations and artifacts.
You can find the location of a pulled repository as follows:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>find<span class="w"> </span>repo<span class="w"> </span>mlcommons@ck
</code></pre></div>
<div class="highlight"><pre><span></span><code>mlcommons@ck,a4705959af8e447a = /home/ubuntu/CM/repos/mlcommons@ck
</code></pre></div>
<h2 id="install-system-dependencies-for-your-platform">Install system dependencies for your platform</h2>
<p>First, you need to install various system dependencies required by the MLPerf inference benchmark.</p>
<p>For this purpose, we have created a cross-platform CM script that will automatically install 
such dependencies based on your OS (Ubuntu, Debian, Red Hat, MacOS ...). </p>
<p>In this case, CM script serves simply as a wrapper with a unified and cross-platform interface
for native scripts that you can find and extend <a href="https://github.com/mlcommons/ck/tree/master/cm-mlops/script/get-sys-utils-cm">here</a>
if some dependencies are missing on your machine - this is a collaborative way to make 
CM scripts portable and interoperable.</p>
<p>You can run this CM scripts as follows (note that you may be asked for a SUDO password on your platform):</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get sys-utils-cm&quot;</span><span class="w"> </span>--quiet
</code></pre></div>
<p>If you think that you have all system dependencies installed,
you can run this script without <code>--quiet</code> flag and type "skip" in the script prompt.</p>
<h2 id="use-cm-to-detect-or-install-python-38">Use CM to detect or install Python 3.8+</h2>
<p>Since we use Python reference implementation of the MLPerf inference benchmark (unoptimized),
we need to detect or install Python 3.8+ (MLPerf requirement). </p>
<p>You need to detect it using the following <a href="https://github.com/mlcommons/ck/blob/master/docs/list_of_scripts.md#get-python3">CM script</a>:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get python&quot;</span><span class="w"> </span>--version_min<span class="o">=</span><span class="m">3</span>.8
</code></pre></div>
<p>Note, that all artifacts (including the above scripts) in MLCommons CM are organized as a database of interconnected components.
They can be found either by their user friendly tags (such as <code>get,python</code>) or aliases (<code>get-python3</code>) and unique identifiers
(<code>5b4e0237da074764</code>).
You can find this information in a <a href="https://github.com/mlcommons/ck/blob/master/cm-mlops/script/get-python3/_cm.json">CM meta description of this script</a>.</p>
<p>If required Python is installed on your system, CM will detect it and cache related environment variables such as PATH, PYTHONPATH, etc.
to be reused by other CM scripts. You can find an associated CM cache entry for your python as follows:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>show<span class="w"> </span>cache<span class="w"> </span>--tags<span class="o">=</span>get,python
</code></pre></div>
<p>You can see the environment variables produced by this CM script in the following JSON file:
<div class="highlight"><pre><span></span><code>cat<span class="w"> </span><span class="sb">`</span>cm<span class="w"> </span>find<span class="w"> </span>cache<span class="w"> </span>--tags<span class="o">=</span>get,python<span class="sb">`</span>/cm-cached-state.json
</code></pre></div></p>
<p>If required Python is not detected, CM will automatically attempt to download and build it from sources 
using another <a href="https://github.com/mlcommons/ck/blob/master/docs/list_of_scripts.md#install-python-src">cross-platform CM script "install-python-src"</a>.
In the end, CM will also cache new binaries and related environment variables such as PATH, PYTHONPATH, etc:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>show<span class="w"> </span>cache
</code></pre></div>
<p>You can find installed binaries and reuse them in your own project with or without CM as follows:
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>find<span class="w"> </span>cache<span class="w"> </span>--tags<span class="o">=</span>install,python
</code></pre></div></p>
<p>Note that if you run the same script again, CM will automatically find and reuse the cached output:
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get python&quot;</span><span class="w"> </span>--version_min<span class="o">=</span><span class="m">3</span>.8<span class="w"> </span>--out<span class="o">=</span>json
</code></pre></div></p>
<h2 id="setup-a-virtual-environment-for-python">Setup a virtual environment for Python</h2>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;install python-venv&quot;</span><span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">CM_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
<h2 id="pull-mlperf-inference-sources">Pull MLPerf inference sources</h2>
<p>You should now download and cache the MLPerf inference sources using the following command:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get mlperf inference src&quot;</span>
</code></pre></div>
<h2 id="compile-mlperf-loadgen">Compile MLPerf loadgen</h2>
<p>You need to compile loadgen from the above inference sources:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get mlperf loadgen&quot;</span>
</code></pre></div>
<h1 id="cm-automation-for-the-mlperf-benchmark">CM automation for the MLPerf benchmark</h1>
<h2 id="mlperf-inference-python-bert-fp32-squad-v11-onnx-cpu-offline">MLPerf inference - Python - Bert FP32 - SQUAD v1.1 - ONNX - CPU - Offline</h2>
<h3 id="download-the-squad-dataset">Download the SQuAD dataset</h3>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get dataset squad original&quot;</span>
</code></pre></div>
<p>After installing this dataset via CM, you can reuse it in your own projects or other CM scripts (including MLPerf benchmarks).
You can check the CM cache as follows (the unique ID of the CM cache entry will be different on your machine):
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>show<span class="w"> </span>cache<span class="w"> </span>--tags<span class="o">=</span>get,dataset,squad,original
</code></pre></div></p>
<div class="highlight"><pre><span></span><code>* Tags: dataset,get,language-processing,original,script-artifact-6651c119c3ae49b3,squad,validation,version-1.1
  Path: /home/arjun/CM/repos/local/cache/e5ac8a524ba64d09
  Version: 1.1
</code></pre></div>
<h3 id="install-onnx-runtime-for-cpu">Install ONNX runtime for CPU</h3>
<p>Now detect or install ONNX Python runtime (targeting CPU) your system
using a <a href="https://github.com/mlcommons/ck/blob/master/docs/list_of_scripts.md#get-generic-python-lib">generic CM script</a> to install python package:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get generic-python-lib _onnxruntime&quot;</span>
</code></pre></div>
<h3 id="download-bert-large-model-fp32-onnx-format">Download Bert-large model (FP32, ONNX format)</h3>
<p>Download and cache this <a href="https://paperswithcode.com/model/resnext?variant=resnext-50-32x4d">reference model</a> in the ONNX format (float32)
using the following <a href="https://github.com/mlcommons/ck/blob/master/docs/list_of_scripts.md#get-ml-model-retinanet">CM script</a>:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;get ml-model language-processing bert-large _onnx&quot;</span>
</code></pre></div>
<p>It takes around ~1GB of disk space. You can find it in the CM cache as follows:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>show<span class="w"> </span>cache<span class="w"> </span>--tags<span class="o">=</span>get,ml-model,bert-large,_onnx
</code></pre></div>
<div class="highlight"><pre><span></span><code>*Tags: bert,bert-large,bert-squad,get,language,language-processing,ml-model,raw,script-artifact-5e865dbdc65949d2,_amazon-s3,_fp32,_onnx
  Path: /home/arjun/CM/repos/local/cache/8727a38b72aa4b3f
</code></pre></div>
<h3 id="run-reference-mlperf-inference-benchmark-offline-accuracy">Run reference MLPerf inference benchmark (offline, accuracy)</h3>
<p>You are now ready to run the <a href="https://github.com/mlcommons/inference/tree/master/vision/classification_and_detection/python">reference (unoptimized) Python implementation</a> 
of the MLPerf vision benchmark with <a href="https://github.com/mlcommons/inference/blob/master/vision/classification_and_detection/python/backend_onnxruntime.py">ONNX backend</a>.</p>
<p>Normally, you would need to go through this <a href="https://github.com/mlcommons/inference/tree/master/vision/classification_and_detection">README.md</a>
and prepare all the dependencies and environment variables manually.</p>
<p>The <a href="https://github.com/mlcommons/ck/blob/master/docs/list_of_scripts.md#app-mlperf-inference">CM "app-mlperf-inference" script</a>
allows you to run this benchmark as follows:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;app mlperf inference generic _python _bert-99 _onnxruntime _cpu&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--mode<span class="o">=</span>accuracy<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--test_query_count<span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--rerun
</code></pre></div>
<p>This CM script will automatically find or install all dependencies
described in its <a href="https://github.com/mlcommons/ck/blob/master/cm-mlops/script/app-mlperf-inference/_cm.yaml">CM meta description</a>,
aggregate all environment variables, preprocess all files and assemble the MLPerf benchmark CMD. After running the benchmark, it calls the MLPerf accuracy script to evaluate the accuracy of the results. </p>
<p>It will take a few minutes to run it and you should see the following accuracy:</p>
<div class="highlight"><pre><span></span><code>{&quot;exact_match&quot;: 70.0, &quot;f1&quot;: 70.0}
Reading examples...
No cached features at &#39;eval_features.pickle&#39;... converting from examples...
Creating tokenizer...
Converting examples to features...
Caching features at &#39;eval_features.pickle&#39;...
Loading LoadGen logs...
Post-processing predictions..
</code></pre></div>
<p>Congratulations, you can now play with this benchmark using the unified CM commands!</p>
<p>Note that even if did not install all the above dependencies manually, the below command
will automatically install all the necessary dependencies.</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;app mlperf inference generic _python _bert-99 _onnxruntime _cpu&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--adr.python.version_min<span class="o">=</span><span class="m">3</span>.8<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--adr.compiler.tags<span class="o">=</span>gcc<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--mode<span class="o">=</span>accuracy<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--test_query_count<span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--rerun
</code></pre></div>
<h3 id="run-mlperf-inference-benchmark-offline-performance">Run MLPerf inference benchmark (offline, performance)</h3>
<p>Let's run the MLPerf object detection while measuring performance:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span><span class="s2">&quot;app mlperf inference generic _python _bert-99 _onnxruntime _cpu&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--mode<span class="o">=</span>performance<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--rerun
</code></pre></div>
<p>It will run for a few seconds and you should see an output similar to the following one at the end
(the QPS is the performance result of this benchmark that depends on the speed of your system):</p>
<div class="highlight"><pre><span></span><code>================================================
MLPerf Results Summary
================================================
SUT name : PySUT
Scenario : Offline
Mode     : PerformanceOnly
Samples per second: 3.71597
Result is : VALID
  Min duration satisfied : Yes
  Min queries satisfied : Yes
  Early stopping satisfied: Yes

================================================
Additional Stats
================================================
Min latency (ns)                : 267223684
Max latency (ns)                : 2691085775
Mean latency (ns)               : 1478150052
50.00 percentile latency (ns)   : 1612665856
90.00 percentile latency (ns)   : 2691085775
95.00 percentile latency (ns)   : 2691085775
97.00 percentile latency (ns)   : 2691085775
99.00 percentile latency (ns)   : 2691085775
99.90 percentile latency (ns)   : 2691085775

================================================
Test Parameters Used
================================================
samples_per_query : 10
target_qps : 1
target_latency (ns): 0
max_async_queries : 1
min_duration (ms): 0
max_duration (ms): 0
min_query_count : 1
max_query_count : 10
qsl_rng_seed : 148687905518835231
sample_index_rng_seed : 520418551913322573
schedule_rng_seed : 811580660758947900
accuracy_log_rng_seed : 0
accuracy_log_probability : 0
accuracy_log_sampling_target : 0
print_timestamps : 0
performance_issue_unique : 0
performance_issue_same : 0
performance_issue_same_index : 0
performance_sample_count : 10833

No warnings encountered during test.

No errors encountered during test.
</code></pre></div>
<p>Note that QPS is very low because we use an unoptimized reference implementation of this benchmark on CPU.</p>
<h3 id="prepare-mlperf-submission">Prepare MLPerf submission</h3>
<p>You are now ready to generate the submission similar to the ones appearing
on the <a href="https://mlcommons.org/en/inference-edge-21">official MLPerf inference dashboard</a>.</p>
<p>We have developed another script that runs the MLPerf inference benchmark in both accuracy and performance mode,
runs the submission checker, unifies output for a dashboard and creates a valid MLPerf submission pack in <code>mlperf_submission.tar.gz</code> 
with all required MLPerf logs and stats.</p>
<p>You can run this script as follows:</p>
<div class="highlight"><pre><span></span><code>cm<span class="w"> </span>run<span class="w"> </span>script<span class="w"> </span>--tags<span class="o">=</span>run,mlperf,inference,run-mlperf,_submission,_short<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--submitter<span class="o">=</span><span class="s2">&quot;Community&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--hw_name<span class="o">=</span>default<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--backend<span class="o">=</span>onnxruntime<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--test_query_count<span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--clean
</code></pre></div>
<p>It will take a few minutes to run and you should see the following output in the end:</p>
<div class="highlight"><pre><span></span><code>[2023-09-26 19:20:42,245 submission_checker1.py:3308 INFO] Results open/Community/results/default-reference-cpu-onnxruntime-v1.15.1-default_config/bert-99/offline 3.72101
[2023-09-26 19:20:42,245 submission_checker1.py:3310 INFO] ---
[2023-09-26 19:20:42,245 submission_checker1.py:3395 INFO] ---
[2023-09-26 19:20:42,245 submission_checker1.py:3396 INFO] Results=1, NoResults=0, Power Results=0
[2023-09-26 19:20:42,245 submission_checker1.py:3403 INFO] ---
[2023-09-26 19:20:42,245 submission_checker1.py:3404 INFO] Closed Results=0, Closed Power Results=0

[2023-09-26 19:20:42,245 submission_checker1.py:3409 INFO] Open Results=1, Open Power Results=0

[2023-09-26 19:20:42,245 submission_checker1.py:3414 INFO] Network Results=0, Network Power Results=0

[2023-09-26 19:20:42,245 submission_checker1.py:3419 INFO] ---
[2023-09-26 19:20:42,245 submission_checker1.py:3421 INFO] Systems=1, Power Systems=0
[2023-09-26 19:20:42,245 submission_checker1.py:3422 INFO] Closed Systems=0, Closed Power Systems=0
[2023-09-26 19:20:42,245 submission_checker1.py:3427 INFO] Open Systems=1, Open Power Systems=0
[2023-09-26 19:20:42,245 submission_checker1.py:3432 INFO] Network Systems=0, Network Power Systems=0
[2023-09-26 19:20:42,245 submission_checker1.py:3437 INFO] ---
[2023-09-26 19:20:42,245 submission_checker1.py:3442 INFO] SUMMARY: submission looks OK
/usr/bin/python3 /home/arjun/CM/repos/local/cache/0cfdde8a3bd64fb6/inference/tools/submission/generate_final_report.py --input summary.csv
=========================================================
Searching for summary.csv ...
Converting to json ...

                                                                           0
Organization                                                       Community
Availability                                                       available
Division                                                                open
SystemType                                                              edge
SystemName                                                           default
Platform                   default-reference-cpu-onnxruntime-v1.15.1-defa...
Model                                                                bert-99
MlperfModel                                                          bert-99
Scenario                                                             Offline
Result                                                               3.72101
Accuracy                                                                70.0
number_of_nodes                                                            1
host_processor_model_name                AMD Ryzen 9 7950X 16-Core Processor
host_processors_per_node                                                   1
host_processor_core_count                                                 16
accelerator_model_name                                                   NaN
accelerators_per_node                                                      0
Location                   open/Community/results/default-reference-cpu-o...
framework                                                onnxruntime v1.15.1
operating_system             Ubuntu 22.04 (linux-6.2.0-32-generic-glibc2.35)
notes                      Powered by MLCommons Collective Mind framework...
compliance                                                                 1
errors                                                                     0
version                                                                 v3.1
inferred                                                                   0
has_power                                                              False
Units                                                              Samples/s
</code></pre></div>
<p>Note that <code>--clean</code> flag cleans all previous runs of MLPerf benchmark to make sure that the MLPerf submission script picks up the latest results.</p>
<p>You will also see the following 3 files in your current directory:
<div class="highlight"><pre><span></span><code>ls -l
mlperf_submission.tar.gz
summary.csv
summary.json
</code></pre></div></p>
<p>Note that by default, CM-MLPerf will store the raw results 
in <code>$HOME/mlperf_submission</code> (with truncated accuracy logs) and in <code>$HOME/mlperf_submission_logs</code> 
(with complete and very large accuracy logs).</p>
<p>You can change this directory using the flag <code>--submission_dir={directory to store raw MLPerf results}</code>
in the above script.</p>
<h2 id="trying-deepsparse-backend">Trying deepsparse backend</h2>
<h3 id="int8">int8</h3>
<div class="highlight"><pre><span></span><code>cm run script --tags=run,mlperf,inference,run-mlperf,_submission,_short  \
   --implementation=reference \
   --model=bert-99 \
   --backend=deepsparse \
   --device=cpu \
   --scenario=Offline \
   --test_query_count=1024 \
   --adr.mlperf-inference-implementation.max_batchsize=128 \
   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni \
   --clean 
</code></pre></div>
<h3 id="fp32">fp32</h3>
<div class="highlight"><pre><span></span><code>cm run script --tags=run,mlperf,inference,run-mlperf,_submission,_short  \
   --adr.python.version_min=3.8 \
   --implementation=reference \
   --model=bert-99 \
   --backend=deepsparse \
   --device=cpu \
   --scenario=Offline \
   --test_query_count=1024 \
   --adr.mlperf-inference-implementation.max_batchsize=128 \
   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni \
   --clean 
</code></pre></div>
<h2 id="questions-suggestions">Questions? Suggestions?</h2>
<p>Check the <a href="../../../../taskforce/">MLCommons Task Force on Automation and Reproducibility</a> 
and get in touch via <a href="https://discord.gg/JjWNWXKxwT">public Discord server</a>.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../..", "features": ["content.tabs.link", "content.code.copy", "navigation.expand", "navigation.sections", "navigation.indexes", "navigation.instant", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "toc.follow"], "search": "../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
    
  </body>
</html>